{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b47188",
   "metadata": {},
   "source": [
    "## Import libraries for data loading, modeling, and visualization\n",
    "We bring in PyTorch for building neural networks, torchvision for the MNIST dataset and image transforms, and matplotlib so we can visualize digits later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90027b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bbbf1",
   "metadata": {},
   "source": [
    "## Download the MNIST datasets and convert them to tensors\n",
    "MNIST is a collection of 28x28 grayscale images of handwritten digits (0â€“9). We download both the training and test splits and immediately convert each image into a PyTorch tensor so the model can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2036a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\", train=True, transform=ToTensor(), download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\", train=False, transform=ToTensor(), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af153e3e",
   "metadata": {},
   "source": [
    "## Double-check the tensor shape of the training set\n",
    "Inspecting the shape confirms how many training samples we have and ensures each image is 28x28 pixels with a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a424a6-7b8b-4e6f-a597-8153468a3236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7a344",
   "metadata": {},
   "source": [
    "## Verify the tensor shape of the test set\n",
    "The test set should mirror the same structure so evaluation uses data identical to the training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bb27e5-31a0-4a97-afad-d5c3e0c48706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265e641",
   "metadata": {},
   "source": [
    "## Inspect the label tensor to confirm class coverage\n",
    "Viewing the labels helps verify that all digits are present and that labels align with the image tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe9a058-6e99-40f1-8455-e5ad96eb7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a3aca",
   "metadata": {},
   "source": [
    "## Build PyTorch data loaders for training and evaluation\n",
    "DataLoaders handle shuffling, batching, and parallel loading, which keeps the training loop clean and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28758450-a7b4-49c9-913c-4a242e08ce72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    \"train\": DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    \"test\": DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2eb0f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the convolutional neural network used for MNIST classification\n",
    "This class builds a small CNN: two convolutional layers capture image patterns, followed by fully connected layers that map the extracted features to digit predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe46dac-eb4f-43fb-a876-0863d0338470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f4eff",
   "metadata": {},
   "source": [
    "## Configure device, instantiate the model, and define optimizer, loss, and loops\n",
    "We move everything to GPU if available, create the CNN, choose the Adam optimizer to adjust weights, and set cross-entropy as the loss to measure prediction quality against labels. The `train` helper runs one pass through the shuffled training batches, while `test` switches the model to evaluation mode and measures accuracy on unseen data without updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c9e77a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_STATE_PATH = Path(\"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders['train'].dataset)} ({100.0 * batch_idx / len(loaders['train']):.0f}%)]\\t{loss.item():.6}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders[\"test\"]:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders[\"test\"].dataset)\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(loaders['test'].dataset)} ({100.0 * correct / len(loaders['test'].dataset):.0f}%\\n)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2044f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Load existing weights or train for multiple epochs\n",
    "If a saved checkpoint exists we ask whether to reuse it; otherwise we loop through the dataset ten times, logging accuracy after each epoch before persisting the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d9d3ef-a943-4d95-bb3a-5d128b140809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Existing weights found at mnist_cnn.pt. Press Enter to use them, or type 'r' to retrain:  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained weights from mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "def run_training_flow():\n",
    "    retrain = True\n",
    "    if MODEL_STATE_PATH.exists():\n",
    "        choice = (\n",
    "            input(\n",
    "                f\"Existing weights found at {MODEL_STATE_PATH}. \"\n",
    "                \"Press Enter to use them, or type 'r' to retrain: \"\n",
    "            )\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "        retrain = choice == \"r\"\n",
    "\n",
    "    if retrain:\n",
    "        print(f\"Model is currently being trained using {str(device).upper()}\")\n",
    "        time.sleep(1)\n",
    "        for epoch in range(1, 11):\n",
    "            train(epoch)\n",
    "            test()\n",
    "        torch.save(model.state_dict(), MODEL_STATE_PATH)\n",
    "        print(f\"Saved trained weights to {MODEL_STATE_PATH}\")\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(MODEL_STATE_PATH, map_location=device))\n",
    "        model.to(device)\n",
    "        print(f\"Loaded trained weights from {MODEL_STATE_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628e35a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Run a sample inference and visualize the corresponding digit\n",
    "After training, we grab a user-selected test image (defaults to index 0 when no input is provided), run it through the model to get a prediction, and display the digit so we can visually verify the result makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303f6914-8457-41b3-b255-0ae49160a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter test sample index (0-9999, default 0):  50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for sample 50: 6\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Display the digit with matplotlib? [y/N]:  y\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Classify another test sample? [y/N]:  y\n",
      "Enter test sample index (0-9999, default 0):  70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for sample 70: 7\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Display the digit with matplotlib? [y/N]:  y\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Classify another test sample? [y/N]:  \n"
     ]
    }
   ],
   "source": [
    "def run_sample_inference():\n",
    "    model.eval()\n",
    "\n",
    "    default_sample_index = 0\n",
    "\n",
    "    max_index = len(test_data) - 1\n",
    "\n",
    "    while True:\n",
    "        user_choice = input(\n",
    "            f\"Enter test sample index (0-{max_index}, default {default_sample_index}): \"\n",
    "        ).strip()\n",
    "\n",
    "        if user_choice:\n",
    "            try:\n",
    "                sample_index = int(user_choice)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input, defaulting to index 0.\")\n",
    "                sample_index = default_sample_index\n",
    "        else:\n",
    "            sample_index = default_sample_index\n",
    "\n",
    "        sample_index = max(0, min(sample_index, max_index))\n",
    "\n",
    "        data, target = test_data[sample_index]\n",
    "\n",
    "        data = data.unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "        print(f\"Prediction for sample {sample_index}: {prediction}\")\n",
    "\n",
    "        show_choice = input(\"Display the digit with matplotlib? [y/N]: \").strip().lower()\n",
    "        if show_choice in (\"y\", \"yes\"):\n",
    "            image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "            plt.imshow(image, cmap=\"gray\")\n",
    "            plt.show()\n",
    "\n",
    "        repeat_choice = input(\"Classify another test sample? [y/N]: \").strip().lower()\n",
    "        if repeat_choice not in (\"y\", \"yes\"):\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_sample_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bb777-cd41-407d-9b0f-33c5110c7211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-uppgift01",
   "language": "python",
   "name": "python-uppgift01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
