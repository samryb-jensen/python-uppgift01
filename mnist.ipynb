{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b47188",
   "metadata": {},
   "source": [
    "## Import libraries for data loading, modeling, and visualization\n",
    "We bring in PyTorch for building neural networks, torchvision for the MNIST dataset and image transforms, and matplotlib so we can visualize digits later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90027b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bbbf1",
   "metadata": {},
   "source": [
    "## Download the MNIST datasets and convert them to tensors\n",
    "MNIST is a collection of 28x28 grayscale images of handwritten digits (0â€“9). We download both the training and test splits and immediately convert each image into a PyTorch tensor so the model can work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2036a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\", train=True, transform=ToTensor(), download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\", train=False, transform=ToTensor(), download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af153e3e",
   "metadata": {},
   "source": [
    "## Double-check the tensor shape of the training set\n",
    "Inspecting the shape confirms how many training samples we have and ensures each image is 28x28 pixels with a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a424a6-7b8b-4e6f-a597-8153468a3236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff7a344",
   "metadata": {},
   "source": [
    "## Verify the tensor shape of the test set\n",
    "The test set should mirror the same structure so evaluation uses data identical to the training format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03bb27e5-31a0-4a97-afad-d5c3e0c48706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265e641",
   "metadata": {},
   "source": [
    "## Inspect the label tensor to confirm class coverage\n",
    "Viewing the labels helps verify that all digits are present and that labels align with the image tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe9a058-6e99-40f1-8455-e5ad96eb7aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a3aca",
   "metadata": {},
   "source": [
    "## Build PyTorch data loaders for training and evaluation\n",
    "DataLoaders handle shuffling, batching, and parallel loading, which keeps the training loop clean and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28758450-a7b4-49c9-913c-4a242e08ce72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    \"train\": DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    \"test\": DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2eb0f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define the convolutional neural network used for MNIST classification\n",
    "This class builds a small CNN: two convolutional layers capture image patterns, followed by fully connected layers that map the extracted features to digit predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe46dac-eb4f-43fb-a876-0863d0338470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f4eff",
   "metadata": {},
   "source": [
    "## Configure device, instantiate the model, and define optimizer, loss, and loops\n",
    "We move everything to GPU if available, create the CNN, choose the Adam optimizer to adjust weights, and set cross-entropy as the loss to measure prediction quality against labels. The `train` helper runs one pass through the shuffled training batches, while `test` switches the model to evaluation mode and measures accuracy on unseen data without updating weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c9e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "MODEL_STATE_PATH = Path(\"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loaders[\"train\"]):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(loaders['train'].dataset)} ({100.0 * batch_idx / len(loaders['train']):.0f}%)]\\t{loss.item():.6}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders[\"test\"]:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(loaders[\"test\"].dataset)\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy {correct}/{len(loaders['test'].dataset)} ({100.0 * correct / len(loaders['test'].dataset):.0f}%\\n)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2044f4",
   "metadata": {},
   "source": [
    "## Load existing weights or train for multiple epochs\n",
    "If a saved checkpoint exists we load it and skip training; otherwise we loop through the dataset ten times, logging accuracy after each epoch before persisting the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d9d3ef-a943-4d95-bb3a-5d128b140809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained weights from mnist_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "if MODEL_STATE_PATH.exists():\n",
    "    model.load_state_dict(torch.load(MODEL_STATE_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"Loaded trained weights from {MODEL_STATE_PATH}\")\n",
    "else:\n",
    "    for epoch in range(1, 11):\n",
    "        train(epoch)\n",
    "        test()\n",
    "    torch.save(model.state_dict(), MODEL_STATE_PATH)\n",
    "    print(f\"Saved trained weights to {MODEL_STATE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628e35a",
   "metadata": {},
   "source": [
    "## Quickly confirm whether training ran on CPU or GPU\n",
    "Printing the `device` lets us confirm whether we benefitted from GPU acceleration or fell back to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "303f6914-8457-41b3-b255-0ae49160a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5c359",
   "metadata": {},
   "source": [
    "## Run a sample inference and visualize the corresponding digit\n",
    "After training, we grab a user-selected test image (defaults to index 0 when running inside Jupyter), run it through the model to get a prediction, and display the digit so we can visually verify the result makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b6520d-dbe2-4378-a601-a3a571c32eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter test sample index (0-9999, default 0):  76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for sample 76: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "default_sample_index = 0\n",
    "\n",
    "max_index = len(test_data) - 1\n",
    "user_choice = input(\n",
    "    f\"Enter test sample index (0-{max_index}, default {default_sample_index}): \"\n",
    ").strip()\n",
    "\n",
    "if user_choice:\n",
    "    try:\n",
    "        sample_index = int(user_choice)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input, defaulting to index 0.\")\n",
    "        sample_index = default_sample_index\n",
    "else:\n",
    "    sample_index = default_sample_index\n",
    "\n",
    "sample_index = max(0, min(sample_index, max_index))\n",
    "\n",
    "data, target = test_data[sample_index]\n",
    "\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f\"Prediction for sample {sample_index}: {prediction}\")\n",
    "\n",
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bb777-cd41-407d-9b0f-33c5110c7211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-uppgift01",
   "language": "python",
   "name": "python-uppgift01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
